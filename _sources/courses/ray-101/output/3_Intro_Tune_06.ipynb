{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter tuning the PyTorch model using Ray Tune\n",
    "\n",
    "The first step is to move in all the PyTorch code into a function that we can pass to the `trainable` argument of the `tune.run` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pytorch(config): # we change the function so it accepts a config dictionary\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    model = resnet18()\n",
    "    model.conv1 = torch.nn.Conv2d(\n",
    "        1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
    "    )\n",
    "    model.to(\"cuda\")\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=config[\"lr\"])\n",
    "    \n",
    "    transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "    train_data = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "    data_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True)\n",
    "\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Report the metrics using train.report instead of print\n",
    "        train.report({\"loss\": loss.item()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second and third steps are the same as before. We define the tuner and run it by calling the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-11-29 09:44:37</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:25.36        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.6/31.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/16 CPUs, 1.0/2 GPUs (0.0/1.0 anyscale/node-group:1xT4:8CPU-32GB, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/region:us-west-2, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/1.0 anyscale/node-group:head)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name               </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">        lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">     loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_pytorch_7cf0c_00000</td><td>TERMINATED</td><td>10.0.35.222:38448</td><td style=\"text-align: right;\">0.0582891 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         19.9064</td><td style=\"text-align: right;\">0.264131 </td></tr>\n",
       "<tr><td>train_pytorch_7cf0c_00001</td><td>TERMINATED</td><td>10.0.20.35:10307 </td><td style=\"text-align: right;\">0.00787705</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         20.2805</td><td style=\"text-align: right;\">0.0564279</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]\n",
      "  1%|          | 98304/9912422 [00:00<00:11, 831953.25it/s]\n",
      "  4%|\u258d         | 393216/9912422 [00:00<00:05, 1806326.67it/s]\n",
      " 16%|\u2588\u258c        | 1572864/9912422 [00:00<00:01, 5521900.47it/s]\n",
      " 62%|\u2588\u2588\u2588\u2588\u2588\u2588\u258f   | 6127616/9912422 [00:00<00:00, 18478655.81it/s]\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 18409805.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00<00:00, 507557.46it/s]\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9912422/9912422 [00:00<00:00, 16263535.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00<00:00, 4649632.58it/s]\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28881/28881 [00:00<00:00, 490189.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\u001b[32m [repeated 12x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_pytorch pid=38448)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m Failed to download (trying next):\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m HTTP Error 403: Forbidden\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m \n",
      "\u001b[36m(train_pytorch pid=10307, ip=10.0.20.35)\u001b[0m Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00<00:00, 3523960.19it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00<00:00, 4641500.47it/s]\u001b[32m [repeated 10x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 09:44:37,315\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/ray/ray_results/train_pytorch_2024-11-29_09-44-11' in 0.0033s.\n",
      "2024-11-29 09:44:37,319\tINFO tune.py:1041 -- Total run time: 25.37 seconds (25.36 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "tuner = tune.Tuner(\n",
    "    trainable=tune.with_resources(train_pytorch, {\"gpu\": 1}), # we will dedicate 1 GPU to each trial\n",
    "    param_space={\n",
    "        \"num_epochs\": 1,\n",
    "        \"batch_size\": 128,\n",
    "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        mode=\"min\",\n",
    "        metric=\"loss\",\n",
    "        num_samples=2,\n",
    "        search_alg=tune.search.BasicVariantGenerator(),\n",
    "        scheduler=tune.schedulers.FIFOScheduler(),\n",
    "    ),\n",
    ")\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can get the best result and its configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_epochs': 1, 'batch_size': 128, 'lr': 0.007877049646500664}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result = results.get_best_result()\n",
    "best_result.config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}