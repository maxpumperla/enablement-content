{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Launching the distributed training job\n",
    "\n",
    "<img src=\"https://anyscale-public-materials.s3.us-west-2.amazonaws.com/ray-ai-libraries/diagrams/multi_gpu_pytorch_annotated_v5.png\" width=\"1000\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's proceed to launch the distributed training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure persistent storage\n",
    "Create a `RunConfig` object to specify the path where results (including checkpoints and artifacts) will be saved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import RunConfig\n",
    "\n",
    "storage_path = \"/mnt/cluster_storage/ray-summit-2024-training/\"\n",
    "run_config = RunConfig(storage_path=storage_path, name=\"distributed-mnist-resnet18\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching a distributed training job with a `TorchTrainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 11:46:34,891\tINFO worker.py:1596 -- Connecting to existing Ray cluster at address: 10.0.25.5:6379...\n",
      "2024-11-22 11:46:34,898\tINFO worker.py:1772 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://session-wun39fg7yb3g9682a8fejskwz3.i.anyscaleuserdata.com \u001b[39m\u001b[22m\n",
      "2024-11-22 11:46:35,055\tINFO packaging.py:358 -- Pushing file package 'gcs://_ray_pkg_c0d0f875c8f886002dd044911a6c5a1d734f1c8c.zip' (63.64MiB) to Ray cluster...\n",
      "2024-11-22 11:46:35,772\tINFO packaging.py:371 -- Successfully pushed file package 'gcs://_ray_pkg_c0d0f875c8f886002dd044911a6c5a1d734f1c8c.zip'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(autoscaler +1h34m20s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n"
     ]
    }
   ],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_ray_train,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    train_loop_config={\"num_epochs\": 2, \"global_batch_size\": 128},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `trainer.fit()` will start the run and block until it completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 11:49:35,784\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-11-22 11:49:35 (running for 00:00:00.11)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/region:us-west-2)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:49:40 (running for 00:00:05.12)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/region:us-west-2)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:49:45 (running for 00:00:10.15)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/region:us-west-2, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:49:50 (running for 00:00:15.16)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/region:us-west-2, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:49:55 (running for 00:00:20.20)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 anyscale/region:us-west-2)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:50:01 (running for 00:00:25.22)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 anyscale/region:us-west-2)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:50:06 (running for 00:00:30.26)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/region:us-west-2, 0.0/2.0 anyscale/provider:aws)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:50:11 (running for 00:00:35.28)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/region:us-west-2, 0.0/2.0 anyscale/provider:aws)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:50:16 (running for 00:00:40.31)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/region:us-west-2)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:50:21 (running for 00:00:45.34)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/accelerator_shape:1xT4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/region:us-west-2)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n",
      "== Status ==\n",
      "Current time: 2024-11-22 11:50:26 (running for 00:00:50.37)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/region:us-west-2, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m Failed to download (trying next):\n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m HTTP Error 403: Forbidden\n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9912422 [00:00<?, ?it/s]38)\u001b[0m \n",
      "  1%|          | 98304/9912422 [00:00<00:11, 875444.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m [rank1]:[W1122 11:49:45.059424410 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\u001b[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00<00:00, 4945514.39it/s]\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1648877/1648877 [00:00<00:00, 4100558.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m Moving model to device: cuda:0\n",
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m Wrapping provided model in DistributedDataParallel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m {'loss': 0.30887049436569214, 'accuracy': 0.7834368348121643, 'epoch': 0}\n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m Failed to download (trying next):\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m HTTP Error 403: Forbidden\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray-summit-2024-training/distributed-mnist-resnet18/TorchTrainer_d89d0_00000_0_2024-11-22_11-49-35/checkpoint_000000)\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4542/4542 [00:00<00:00, 3756759.76it/s]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      " 38%|\u2588\u2588\u2588\u258a      | 622592/1648877 [00:00<00:00, 2175211.11it/s]\u001b[32m [repeated 13x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=30603, ip=10.0.4.138)\u001b[0m {'loss': 0.06367093324661255, 'accuracy': 0.9425080418586731, 'epoch': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=129887)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/mnt/cluster_storage/ray-summit-2024-training/distributed-mnist-resnet18/TorchTrainer_d89d0_00000_0_2024-11-22_11-49-35/checkpoint_000001)\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 11:50:29,707\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/mnt/cluster_storage/ray-summit-2024-training/distributed-mnist-resnet18' in 0.0255s.\n",
      "2024-11-22 11:50:29,709\tINFO tune.py:1041 -- Total run time: 53.92 seconds (53.89 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2024-11-22 11:50:29 (running for 00:00:53.91)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 1.0/16 CPUs, 2.0/2 GPUs (0.0/2.0 anyscale/region:us-west-2, 0.0/2.0 accelerator_type:T4, 0.0/2.0 anyscale/provider:aws, 0.0/2.0 anyscale/accelerator_shape:1xT4)\n",
      "Result logdir: /tmp/ray/session_2024-11-22_06-00-05_069833_2422/artifacts/2024-11-22_11-49-35/distributed-mnist-resnet18/driver_artifacts\n",
      "Number of trials: 1/1 (1 TERMINATED)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orphan": true,
  "vscode": {
   "interpreter": {
    "hash": "a8c1140d108077f4faeb76b2438f85e4ed675f93d004359552883616a1acd54c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}